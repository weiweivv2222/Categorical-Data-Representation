{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4642394c-7b0b-4e97-9bf2-b6ed812e7d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LR\n",
      "Accuracy: 0.9075 (95% CI: 0.9047 - 0.9103)\n",
      "Precision: 0.6644 (95% CI: 0.6404 - 0.6884)\n",
      "Recall: 0.3621 (95% CI: 0.3319 - 0.3923)\n",
      "F1 Score: 0.4683 (95% CI: 0.4424 - 0.4942)\n",
      "ROC AUC: 0.9141 (95% CI: 0.9088 - 0.9194)\n",
      "Average Computation Time per Fold: 1.2059 seconds\n",
      "\n",
      "Classifier: DT\n",
      "Accuracy: 0.9061 (95% CI: 0.9024 - 0.9098)\n",
      "Precision: 0.6509 (95% CI: 0.6272 - 0.6745)\n",
      "Recall: 0.3591 (95% CI: 0.3263 - 0.3918)\n",
      "F1 Score: 0.4625 (95% CI: 0.4317 - 0.4933)\n",
      "ROC AUC: 0.8515 (95% CI: 0.8411 - 0.8619)\n",
      "Average Computation Time per Fold: 0.5338 seconds\n",
      "\n",
      "Classifier: RF\n",
      "Accuracy: 0.8991 (95% CI: 0.8950 - 0.9031)\n",
      "Precision: 0.6913 (95% CI: 0.6518 - 0.7307)\n",
      "Recall: 0.1864 (95% CI: 0.1479 - 0.2249)\n",
      "F1 Score: 0.2930 (95% CI: 0.2427 - 0.3433)\n",
      "ROC AUC: 0.8933 (95% CI: 0.8863 - 0.9003)\n",
      "Average Computation Time per Fold: 7.4279 seconds\n",
      "\n",
      "Classifier: KNN\n",
      "Accuracy: 0.8884 (95% CI: 0.8855 - 0.8913)\n",
      "Precision: 0.5088 (95% CI: 0.4841 - 0.5335)\n",
      "Recall: 0.2504 (95% CI: 0.2191 - 0.2817)\n",
      "F1 Score: 0.3354 (95% CI: 0.3025 - 0.3683)\n",
      "ROC AUC: 0.6971 (95% CI: 0.6754 - 0.7188)\n",
      "Average Computation Time per Fold: 0.0654 seconds\n",
      "\n",
      "Classifier: XGB\n",
      "Accuracy: 0.9091 (95% CI: 0.9058 - 0.9124)\n",
      "Precision: 0.6234 (95% CI: 0.6029 - 0.6440)\n",
      "Recall: 0.4886 (95% CI: 0.4608 - 0.5164)\n",
      "F1 Score: 0.5476 (95% CI: 0.5278 - 0.5673)\n",
      "ROC AUC: 0.9270 (95% CI: 0.9230 - 0.9310)\n",
      "Average Computation Time per Fold: 1.8565 seconds\n",
      "\n",
      "Classifier: MLP\n",
      "Accuracy: 0.8947 (95% CI: 0.8922 - 0.8972)\n",
      "Precision: 0.5442 (95% CI: 0.5244 - 0.5641)\n",
      "Recall: 0.4106 (95% CI: 0.3600 - 0.4611)\n",
      "F1 Score: 0.4668 (95% CI: 0.4381 - 0.4955)\n",
      "ROC AUC: 0.8983 (95% CI: 0.8920 - 0.9045)\n",
      "Average Computation Time per Fold: 60.1206 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import config_cat_embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "from tqdm.notebook import tqdm\n",
    "from data_prep import bank_data_prep\n",
    "from embedding_helper import create_network\n",
    "from scipy import stats  # For confidence intervals\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and preprocess data\n",
    "data_path = config_cat_embedding.paths['data']\n",
    "data_path_out = config_cat_embedding.paths['data_output']\n",
    "bank_data = pd.read_csv(data_path + 'bank-additional-full.csv', sep=';')\n",
    "\n",
    "df_bank, cat_cols = bank_data_prep(bank_data)\n",
    "\n",
    "X = df_bank.iloc[:, :-1]\n",
    "y = df_bank.y\n",
    "\n",
    "\n",
    "# Define the classifiers\n",
    "seed = 42\n",
    "\n",
    "models = [\n",
    "    ('LR', LogisticRegression(solver='lbfgs', random_state=seed, max_iter=1000)),\n",
    "    ('DT', DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=seed)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=200, max_depth=5, random_state=seed, min_samples_leaf=3)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('XGB', XGBClassifier(eval_metric='logloss', random_state=seed)),\n",
    "    # ('SVM', SVC(gamma='scale', random_state=seed, probability=True)),\n",
    "    ('MLP', KerasClassifier(\n",
    "        model=create_network,\n",
    "        epochs=100, batch_size=100, verbose=0, random_state=seed))\n",
    "]\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # You can adjust n_splits as needed\n",
    "\n",
    "# Function to calculate confidence intervals\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    std_err = stats.sem(data)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "# Move the embedding function outside the loop\n",
    "def get_word2vec_embeddings(df, cat_cols, model, dimpool):\n",
    "    elements = []\n",
    "    for _, row in df.iterrows():\n",
    "        categorical_embeddings = []\n",
    "        for col in cat_cols:\n",
    "            try:\n",
    "                categorical_embeddings.append(model.wv[row[col]])\n",
    "            except KeyError:\n",
    "                categorical_embeddings.append(np.zeros((dimpool,)))\n",
    "        elements.append(np.array(categorical_embeddings))\n",
    "    reshaped_x = np.reshape(elements, (len(elements), len(cat_cols) * dimpool))\n",
    "    return reshaped_x\n",
    "\n",
    "# Main loop over models\n",
    "for name, classifier in models:\n",
    "    print(f\"Classifier: {name}\")\n",
    "    # Lists to store metrics for each fold\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    computation_times = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        # Split data into training and test sets for this fold\n",
    "        X_train_fold = X.iloc[train_index].copy()\n",
    "        X_test_fold = X.iloc[test_index].copy()\n",
    "        y_train_fold = y.iloc[train_index].reset_index(drop=True)\n",
    "        y_test_fold = y.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "        # Combine categorical columns into a single string for Word2Vec\n",
    "        X_train_fold['stringcat'] = X_train_fold[cat_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "        # Train Word2Vec model on training data\n",
    "        dimpool = 30  # Embedding dimension\n",
    "        word2vec_model = Word2Vec(sentences=X_train_fold['stringcat'].str.split(\" \"), vector_size=dimpool,\n",
    "                                  window=2, min_count=1, workers=1, seed=42)\n",
    "\n",
    "        # Generate embeddings for training data\n",
    "        X_train_emb = get_word2vec_embeddings(X_train_fold, cat_cols, word2vec_model, dimpool)\n",
    "\n",
    "        # Handle numerical features\n",
    "        numerical_cols = X_train_fold.select_dtypes(exclude='object').columns.tolist()\n",
    "        X_train_num = X_train_fold[numerical_cols].reset_index(drop=True)\n",
    "\n",
    "        # Create DataFrame for embeddings with string column names\n",
    "        emb_col_names = [f'emb_{i}' for i in range(X_train_emb.shape[1])]\n",
    "        X_train_emb_df = pd.DataFrame(X_train_emb, columns=emb_col_names)\n",
    "\n",
    "        # Concatenate numerical features and embeddings\n",
    "        X_train_combined = pd.concat([X_train_num, X_train_emb_df], axis=1)\n",
    "\n",
    "        # Ensure all column names are strings\n",
    "        X_train_combined.columns = X_train_combined.columns.astype(str)\n",
    "\n",
    "        # Generate embeddings for test data\n",
    "        X_test_emb = get_word2vec_embeddings(X_test_fold, cat_cols, word2vec_model, dimpool)\n",
    "        X_test_num = X_test_fold[numerical_cols].reset_index(drop=True)\n",
    "        X_test_emb_df = pd.DataFrame(X_test_emb, columns=emb_col_names)\n",
    "        X_test_combined = pd.concat([X_test_num, X_test_emb_df], axis=1)\n",
    "        X_test_combined.columns = X_test_combined.columns.astype(str)\n",
    "\n",
    "        # Standard scaling\n",
    "        stc = StandardScaler()\n",
    "        X_train_scaled = stc.fit_transform(X_train_combined)\n",
    "        X_test_scaled = stc.transform(X_test_combined)\n",
    "\n",
    "        # Update number_of_features for MLP\n",
    "        number_of_features = X_train_scaled.shape[1]\n",
    "        if name == 'MLP':\n",
    "            classifier.set_params(model__number_of_features=number_of_features)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        classifier.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        computation_times.append(elapsed_time)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred_fold = classifier.predict(X_test_scaled)\n",
    "        if hasattr(classifier, \"predict_proba\"):\n",
    "            y_pred_prob_fold = classifier.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            y_pred_scores = classifier.decision_function(X_test_scaled)\n",
    "            y_pred_prob_fold = (y_pred_scores - y_pred_scores.min()) / (y_pred_scores.max() - y_pred_scores.min())\n",
    "\n",
    "        # Collect performance metrics\n",
    "        accuracies.append(accuracy_score(y_test_fold, y_pred_fold))\n",
    "        precisions.append(precision_score(y_test_fold, y_pred_fold, zero_division=0))\n",
    "        recalls.append(recall_score(y_test_fold, y_pred_fold))\n",
    "        f1s.append(f1_score(y_test_fold, y_pred_fold))\n",
    "        roc_aucs.append(roc_auc_score(y_test_fold, y_pred_prob_fold))\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    acc_mean, acc_ci_lower, acc_ci_upper = confidence_interval(accuracies)\n",
    "    prec_mean, prec_ci_lower, prec_ci_upper = confidence_interval(precisions)\n",
    "    rec_mean, rec_ci_lower, rec_ci_upper = confidence_interval(recalls)\n",
    "    f1_mean, f1_ci_lower, f1_ci_upper = confidence_interval(f1s)\n",
    "    roc_mean, roc_ci_lower, roc_ci_upper = confidence_interval(roc_aucs)\n",
    "    time_mean = np.mean(computation_times)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {acc_mean:.4f} (95% CI: {acc_ci_lower:.4f} - {acc_ci_upper:.4f})\")\n",
    "    print(f\"Precision: {prec_mean:.4f} (95% CI: {prec_ci_lower:.4f} - {prec_ci_upper:.4f})\")\n",
    "    print(f\"Recall: {rec_mean:.4f} (95% CI: {rec_ci_lower:.4f} - {rec_ci_upper:.4f})\")\n",
    "    print(f\"F1 Score: {f1_mean:.4f} (95% CI: {f1_ci_lower:.4f} - {f1_ci_upper:.4f})\")\n",
    "    print(f\"ROC AUC: {roc_mean:.4f} (95% CI: {roc_ci_lower:.4f} - {roc_ci_upper:.4f})\")\n",
    "    print(f\"Average Computation Time per Fold: {time_mean:.4f} seconds\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
